{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS, ALSModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from cold_start import get_cold_start_rating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a SparkSession object\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"MoviesALS\")\n",
    "         .config(\"spark.driver.host\", \"localhost\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ratings json file into spark dataframe\n",
    "\n",
    "movie_ratings = spark.read.json('data/ratings.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check schema\n",
    "movie_ratings.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast to Pandas dataframe to turn timestamp data to datetime and check nulls. \n",
    "\n",
    "movies_df = movie_ratings.select('*').toPandas()\n",
    "movies_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt to change timestamp object to years, all years are 2000\n",
    "\n",
    "date = pd.to_datetime(movies_df['timestamp'], unit='s').dt.year\n",
    "date.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide to drop timestamp for now because only year 2000\n",
    "\n",
    "movie_ratings = movie_ratings.drop('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test set\n",
    "\n",
    "(training, test) = movie_ratings.randomSplit([.8, .2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ALS instance and fit model\n",
    "\n",
    "als = ALS(maxIter=10,\n",
    "          rank=10,\n",
    "          userCol='user_id',\n",
    "          itemCol='movie_id',\n",
    "          ratingCol='rating',\n",
    "          seed=42)\n",
    "\n",
    "model = als.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Predictions\n",
    "\n",
    "predictions = model.transform(test)\n",
    "predictions.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas dataframe, fill prediction nulls, and convert back to spark dataframe\n",
    "\n",
    "pred_df = predictions.select('*').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_average(user, df):\n",
    "    \"\"\"Return average score for user\"\"\"\n",
    "    user_df = df[df['user_id'] == user]\n",
    "    average = user_df['prediction'].mean()\n",
    "    if np.isnan(average):\n",
    "        return 3\n",
    "    else:\n",
    "        return average\n",
    "    \n",
    "def compute_user_average_if_null(row):\n",
    "    \"\"\"Check if value is null, if so, replace with user average\"\"\"\n",
    "    if np.isnan(row['prediction']):\n",
    "        return user_average(row['user_id'], pred_df)\n",
    "    else:\n",
    "        return row['prediction']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in pred_df[pred_df['prediction'].isna()].iterrows():\n",
    "    pred_df.loc[i, 'prediction'] = get_cold_start_rating(row['user_id'], row['movie_id'])\n",
    "    \n",
    "print(pred_df['prediction'].isna().any())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = spark.createDataFrame(pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model \n",
    "evaluator = RegressionEvaluator(metricName='rmse', labelCol='rating',\n",
    "                               predictionCol='prediction')\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a parameter grid, cross validate for best model with different hyperperameters\n",
    "params_score = {}\n",
    "\n",
    "params = (ParamGridBuilder()\n",
    "          .addGrid(als.regParam, [1, 0.01, 0.001, 0.1])\n",
    "          .addGrid(als.maxIter, [5, 10, 20])\n",
    "          .addGrid(als.rank, [4, 10, 50])).build()\n",
    "\n",
    "cv = CrossValidator(estimator=als, estimatorParamMaps=params, evaluator=evaluator, parallelism=4)\n",
    "\n",
    "best_model = cv.fit(movie_ratings)\n",
    "als_model = best_model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "als_model.save('als_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load requests json file into a spark dataframe\n",
    "    \n",
    "requests = spark.read.json(\"data/requests.json\") \n",
    "\n",
    "# predict requests with als model\n",
    "requests_predictions = model.transform(requests).toPandas()\n",
    "\n",
    "# predict null predictions with cold start model\n",
    "for i, row in requests_predictions[requests_predictions['prediction'].isna()].iterrows():\n",
    "    requests_predictions.loc[i, 'prediction'] = get_cold_start_rating(row['user_id'], row['movie_id'])\n",
    "    \n",
    "print(requests_predictions['prediction'].isna().any())\n",
    "\n",
    "# export request predictions dataframe as json file.\n",
    "predictions = requests_predictions.to_json(r\"data/predictions.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
