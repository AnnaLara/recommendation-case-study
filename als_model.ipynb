{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS, ALSModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a SparkSession object\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"MoviesALS\")\n",
    "         .config(\"spark.driver.host\", \"localhost\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ratings json file into spark dataframe\n",
    "\n",
    "movie_ratings = spark.read.json('data/ratings.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movie_id: long (nullable = true)\n",
      " |-- rating: long (nullable = true)\n",
      " |-- timestamp: double (nullable = true)\n",
      " |-- user_id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check schema\n",
    "movie_ratings.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 719949 entries, 0 to 719948\n",
      "Data columns (total 4 columns):\n",
      "movie_id     719949 non-null int64\n",
      "rating       719949 non-null int64\n",
      "timestamp    719949 non-null float64\n",
      "user_id      719949 non-null int64\n",
      "dtypes: float64(1), int64(3)\n",
      "memory usage: 22.0 MB\n"
     ]
    }
   ],
   "source": [
    "# cast to Pandas dataframe to turn timestamp data to datetime and check nulls. \n",
    "\n",
    "movies_df = movie_ratings.select('*').toPandas()\n",
    "movies_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000    719949\n",
       "Name: timestamp, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attempt to change timestamp object to years, all years are 2000\n",
    "\n",
    "date = pd.to_datetime(movies_df['timestamp'], unit='s').dt.year\n",
    "date.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide to drop timestamp for now because only year 2000\n",
    "\n",
    "movie_ratings = movie_ratings.drop('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test set\n",
    "\n",
    "(training, test) = movie_ratings.randomSplit([.8, .2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ALS instance and fit model\n",
    "\n",
    "als = ALS(maxIter=10,\n",
    "          rank=10,\n",
    "          userCol='user_id',\n",
    "          itemCol='movie_id',\n",
    "          ratingCol='rating')\n",
    "\n",
    "model = als.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[movie_id: bigint, rating: bigint, user_id: bigint, prediction: float]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate Predictions\n",
    "\n",
    "predictions = model.transform(test)\n",
    "predictions.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# Convert to pandas dataframe, fill prediction nulls, and convert back to spark dataframe\n",
    "\n",
    "pred_df = predictions.select('*').toPandas()\n",
    "\n",
    "def user_average(user, df):\n",
    "    \"\"\"Return average score for user\"\"\"\n",
    "    user_df = df[df['user_id'] == user]\n",
    "    average = user_df['prediction'].mean()\n",
    "    if np.isnan(average):\n",
    "        return 3\n",
    "    else:\n",
    "        return average\n",
    "    \n",
    "def compute_user_average_if_null(row):\n",
    "    \"\"\"Check if value is null, if so, replace with user average\"\"\"\n",
    "    if np.isnan(row['prediction']):\n",
    "        return user_average(row['user_id'], pred_df)\n",
    "    else:\n",
    "        return row['prediction']\n",
    "    \n",
    "for i, row in pred_df[pred_df['prediction'].isna()].iterrows():\n",
    "    pred_df.loc[i, 'prediction'] = compute_user_average_if_null(row)\n",
    "    \n",
    "print(pred_df['prediction'].isna().any())\n",
    "    \n",
    "predictions = spark.createDataFrame(pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8770237505561516\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model \n",
    "evaluator = RegressionEvaluator(metricName='rmse', labelCol='rating',\n",
    "                               predictionCol='prediction')\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a parameter grid, cross validate for best model with different hyperperameters\n",
    "params_score = {}\n",
    "\n",
    "params = (ParamGridBuilder()\n",
    "          .addGrid(als.regParam, [1, 0.01, 0.001, 0.1])\n",
    "          .addGrid(als.maxIter, [5, 10, 20])\n",
    "          .addGrid(als.rank, [4, 10, 50])).build()\n",
    "\n",
    "cv = CrossValidator(estimator=als, estimatorParamMaps=params, evaluator=evaluator, parallelism=4)\n",
    "\n",
    "best_model = cv.fit(movie_ratings)\n",
    "als_model = best_model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "als_model.save('als_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load requests json file into a spark dataframe\n",
    "    \n",
    "requests = spark.read.json(\"data/requests.json\") \n",
    "\n",
    "# predict requests with best model\n",
    "requests_predictions = als_model.transform(requests)\n",
    "\n",
    "# export request predictions dataframe as json file.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
